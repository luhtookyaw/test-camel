{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1a43496",
   "metadata": {},
   "source": [
    "### Test Camel Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44865ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, requests\n",
    "from llm import call_llm\n",
    "from psi_to_cactus import convert_psi_file_case_id_to_cactus\n",
    "\n",
    "os.environ[\"NO_PROXY\"] = \"127.0.0.1,localhost\"\n",
    "os.environ[\"no_proxy\"] = os.environ[\"NO_PROXY\"]\n",
    "\n",
    "r = requests.get(\"http://127.0.0.1:8000/v1/models\", timeout=10)\n",
    "print(\"Status:\", r.status_code)\n",
    "print(r.text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e495fc09",
   "metadata": {},
   "source": [
    "## Define requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fde531",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "DATA_PATH = Path(\"data/Patient_PSi_CM_Dataset_Planning_Resistance.json\")\n",
    "CLIENT_PROMPT_PATH = Path(\"prompts/client.txt\")\n",
    "CRITIC_PROMPT_PATH = Path(\"prompts/trust_critic.txt\")\n",
    "MOD_PROMPT_PATH = Path(\"prompts/moderator.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26de13f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "VLLM_SERVER = \"http://127.0.0.1:8000/v1\".strip()\n",
    "CAMEL_MODEL_ID = \"LangAGI-Lab/camel\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5aa2e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# models for your simulation components\n",
    "client_model = \"gpt-4o-mini\"\n",
    "critic_model = \"gpt-4o\"\n",
    "moderator_model = \"gpt-4o\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7caeb06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import (\n",
    "    load_text,\n",
    "    load_patients,\n",
    "    pick_patient,\n",
    "    normalize_patient,\n",
    "    render_template,\n",
    "    parse_trust_score,\n",
    "    parse_yes_no,\n",
    "    format_dialogue,\n",
    "    next_phase,\n",
    "    trust_eval_interval,\n",
    "    print_last_turn,\n",
    "    cactus_to_intake_reason,\n",
    "    trim_camel_history\n",
    ")\n",
    "\n",
    "from llm import call_llm\n",
    "from llm import call_llm_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d7a50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "CASE_ID = \"2-3\"\n",
    "\n",
    "# Load prompts\n",
    "client_template = load_text(CLIENT_PROMPT_PATH)\n",
    "critic_template = load_text(CRITIC_PROMPT_PATH)\n",
    "mod_template = load_text(MOD_PROMPT_PATH)\n",
    "\n",
    "# 1) Load PSI patients and select patient\n",
    "patients = load_patients(data_path=DATA_PATH)\n",
    "patient = pick_patient(patients, patient_id=CASE_ID)\n",
    "p = normalize_patient(patient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb833bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "PSI_JSON_PATH = \"data/Patient_PSi_CM_Dataset_Planning_Resistance.json\"\n",
    "PSI_TO_CACTUS_SYSTEM_PROMPT_PATH = \"prompts/psi_to_cactus_system.txt\"\n",
    "PSI_TO_CACTUS_MODEL = \"gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd0255d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from psi_to_cactus import convert_psi_file_case_id_to_cactus\n",
    "\n",
    "# 2) Convert PSI -> CACTUS (intake-style object)\n",
    "cactus_obj = convert_psi_file_case_id_to_cactus(\n",
    "    psi_json_path=PSI_JSON_PATH,\n",
    "    case_id=CASE_ID,\n",
    "    system_prompt_path=PSI_TO_CACTUS_SYSTEM_PROMPT_PATH,\n",
    "    call_llm_fn=call_llm,\n",
    "    model=PSI_TO_CACTUS_MODEL,\n",
    "    temperature=0.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231e1117",
   "metadata": {},
   "outputs": [],
   "source": [
    "from camel_agent import CamelCounselingSession, CounselorAgent, RESPONSE_PROMPT\n",
    "\n",
    "# 3) Initialize CAMEL therapist session\n",
    "sess = CamelCounselingSession(\n",
    "    vllm_server=VLLM_SERVER,\n",
    "    model_id=CAMEL_MODEL_ID,\n",
    "    temperature=0.7,\n",
    ")\n",
    "intake_form, reason = cactus_to_intake_reason(sess, cactus_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bda4bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Optional: enrich 'p' fields used by your client template\n",
    "# # (This keeps your client simulation consistent with CACTUS-derived information.)\n",
    "# intake = cactus_obj.get(\"intake_form\", {}) or {}\n",
    "# ci = intake.get(\"client_info\", {}) or {}\n",
    "# p[\"name\"] = ci.get(\"name\", p.get(\"name\", \"Client\"))\n",
    "# p[\"history\"] = \"\\n\".join(intake.get(\"past_history\", []) or []) if isinstance(intake.get(\"past_history\"), list) else str(intake.get(\"past_history\", \"\") or \"\")\n",
    "# p[\"situation\"] = cactus_obj.get(\"thought\", p.get(\"situation\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b58c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "# how often to run trust critic\n",
    "interval = trust_eval_interval(p.get(\"resistance_level\"))\n",
    "\n",
    "# phase state\n",
    "phase = \"trust_building\"\n",
    "openness = 1\n",
    "trust_level = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58030b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convo format is your simulator format: assistant=user roles (therapist/client)\n",
    "convo: List[Dict[str, str]] = []\n",
    "turns: List[Dict[str, Any]] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27b5a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Therapist starts (Turn 1 therapist message)\n",
    "# -----------------------\n",
    "therapist_first = \"Hi, it’s nice to meet you. What brings you to therapy today?\"\n",
    "convo.append({\"role\": \"assistant\", \"content\": therapist_first})\n",
    "current_therapist_reply = therapist_first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54165d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TURNS = 30\n",
    "\n",
    "# Used to produce the first therapist reply after planning, without duplicating client message\n",
    "first_reply_generated = False\n",
    "\n",
    "for t in range(1, MAX_TURNS + 1):\n",
    "    # -------- client responds to current therapist prompt\n",
    "    p[\"trust_level\"] = trust_level\n",
    "    p[\"stage_therapy\"] = phase\n",
    "\n",
    "    client_system = render_template(client_template, p)\n",
    "    client_user = (\n",
    "        \"Conversation so far:\\n\"\n",
    "        f\"{format_dialogue(convo, last_n=24)}\\n\\n\"\n",
    "        \"Respond as the client to the therapist's latest message.\"\n",
    "    )\n",
    "\n",
    "    client_text = call_llm_messages(\n",
    "        [{\"role\": \"system\", \"content\": client_system},\n",
    "         {\"role\": \"user\", \"content\": client_user}],\n",
    "        temperature=0.7,\n",
    "        model=client_model,\n",
    "    )\n",
    "    convo.append({\"role\": \"user\", \"content\": client_text})\n",
    "\n",
    "    # -------- critic (openness) — evaluate every N turns\n",
    "    should_eval = (t % interval == 0)\n",
    "    if should_eval:\n",
    "        critic_system = render_template(\n",
    "            critic_template,\n",
    "            {\"dialogue_context\": format_dialogue(convo, last_n=16)},\n",
    "        )\n",
    "        critic_text = call_llm_messages(\n",
    "            [{\"role\": \"system\", \"content\": critic_system}],\n",
    "            model=critic_model,\n",
    "        )\n",
    "        score = parse_trust_score(critic_text)\n",
    "        if score is not None:\n",
    "            openness = score\n",
    "    else:\n",
    "        critic_text = None\n",
    "\n",
    "    # phase progression — ONLY when critic ran\n",
    "    if should_eval:\n",
    "        phase = next_phase(phase, openness)\n",
    "\n",
    "    trust_level = openness\n",
    "\n",
    "    # -------- moderator end? (must decide BEFORE generating next therapist reply)\n",
    "    mod_system = render_template(mod_template, {\"conversation\": format_dialogue(convo, last_n=24)})\n",
    "    mod_text = call_llm_messages([{\"role\": \"system\", \"content\": mod_system}], model=moderator_model)\n",
    "    end_flag = parse_yes_no(mod_text)\n",
    "    if end_flag is None:\n",
    "        end_flag = False\n",
    "\n",
    "    # -------- save ONE combined turn record (therapist prompt + client response)\n",
    "    turns.append({\n",
    "        \"turn_id\": t,\n",
    "        \"phase_for_next_turn\": phase,\n",
    "        \"openness\": openness,\n",
    "        \"therapist\": current_therapist_reply,\n",
    "        \"client\": client_text,\n",
    "        \"critic_raw\": critic_text,\n",
    "        \"moderator_raw\": mod_text,\n",
    "        \"end_session\": end_flag,\n",
    "    })\n",
    "\n",
    "    print_last_turn(convo, t)\n",
    "\n",
    "    # If moderator ends after client, stop here (NO new therapist reply)\n",
    "    if end_flag:\n",
    "        break\n",
    "\n",
    "    # -----------------------\n",
    "    # Therapist generates the NEXT prompt (CAMEL)\n",
    "    # -----------------------\n",
    "    if not first_reply_generated:\n",
    "        # First time: build CAMEL plan using the first client message\n",
    "        sess.start(intake_form=intake_form, reason=reason, first_client_message=client_text)\n",
    "\n",
    "        # Trim ONLY therapist-side history before generating the reply\n",
    "        sess.history = trim_camel_history(sess.history, keep_last=18)        \n",
    "\n",
    "        # Now generate the therapist response to that same client message\n",
    "        # WITHOUT sending the client_text again (prevents duplication)\n",
    "        counselor = CounselorAgent(\n",
    "            sess.vllm_server, \n",
    "            sess.model_id, \n",
    "            sess.cbt_plan,  \n",
    "            RESPONSE_PROMPT,  \n",
    "        )\n",
    "        therapist_reply = counselor.next_utterance(intake_form, reason, sess.history)\n",
    "        sess.history.append({\"role\": \"Counselor\", \"message\": therapist_reply})\n",
    "\n",
    "        # Trim again after appending reply (keeps it bounded forever)\n",
    "        sess.history = trim_camel_history(sess.history, keep_last=18) \n",
    "\n",
    "        first_reply_generated = True\n",
    "    else:\n",
    "        # Normal turns: before calling sess.step(), trim therapist history\n",
    "        sess.history = trim_camel_history(sess.history, keep_last=18)\n",
    "    \n",
    "        therapist_reply = sess.step(client_text)\n",
    "    \n",
    "        # sess.step() appends client+reply internally; trim again to keep bounded\n",
    "        sess.history = trim_camel_history(sess.history, keep_last=18)\n",
    "\n",
    "    convo.append({\"role\": \"assistant\", \"content\": therapist_reply})\n",
    "    current_therapist_reply = therapist_reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1cea4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "final_output = {\n",
    "    \"patient_id\": str(p.get(\"id\", \"\")),\n",
    "    \"patient_name\": str(p.get(\"name\", \"\")),\n",
    "    \"turns\": turns\n",
    "}\n",
    "\n",
    "save_output_path = f\"outputs/{final_output['patient_name']}_{final_output['patient_id']}.json\"\n",
    "\n",
    "Path(save_output_path).write_text(json.dumps(final_output, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "print(f\"Saved: {save_output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
